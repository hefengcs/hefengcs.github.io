---
layout: default
title: "Why should memory and reasoning be unified?"
date: 2025-12-10
author: Feng He
---




# Why should memory and reasoning be unified?
author: "Feng  He"

Dec 10-2025
## Why Do Models Fail at Reasoning?

Modern large language models already possess extremely broad knowledge coverage, yet in situations that require multi-step reasoning, we repeatedly observe the same phenomenon. **The model clearly “knows the answer”, yet still fails to reason correctly. [20-22]**
 It may have access to the relevant knowledge, understand the task context, and even produce a chain of thought that looks reasonable, but the final conclusion is still wrong.

A deeper analysis of these failure cases reveals that the issue does not stem from insufficient logical ability. Instead, it arises from a more fundamental structural cause. **During reasoning, the model cannot reliably activate, maintain, or combine the memories it has already stored.**
 In other words, many reasoning failures are not reasoning failures at all, but failures in memory usage.

Models often make mistakes for the following reasons.

- Inability to activate the correct knowledge at key reasoning steps
- Loss of context during multi-step reasoning
- Failure to integrate long-term knowledge with the current problem
- Generation of reasoning chains that appear coherent but lack consistent memory support

The root of these problems is that **the model’s memory structure is fragmented.**
 Long-term knowledge is buried in parameters, short-term working memory depends on attention and the KV cache, external knowledge comes from RAG, and dialogue history is appended linearly. These sources of memory are independent and cannot be integrated into a continuous and actionable structure.

In human cognition, reasoning is never an isolated process. It is deeply embedded within the memory system. Semantic memory, working memory, and episodic replay jointly form the foundation for reasoning.
 If memory is fragmented, reasoning will inevitably be fragile. Only when memory is organized into an accessible and composable structure can reasoning emerge naturally.

This leads us to a more fundamental question.
 **Should reasoning really be viewed as an independent capability, or has it relied from the very beginning on a unified memory structure.**



## The Fragmentation Problem

When we examine the reasoning behavior of large models more closely, we find a deeper structural issue. **The model’s “memory” is not a unified whole. It is scattered across different subsystems.** As a result, the model cannot form a coherent semantic structure during reasoning, nor can it retrieve relevant information from a unified memory space the way humans do.

In current architectures, models typically rely on four independent sources of memory.

- **Parameters (weights)** store long-term knowledge but cannot be dynamically updated during interaction.
- **Attention and the KV cache** provide short-term working memory but function only within the current context window.
- **Retrieval systems such as RAG or databases** supplement knowledge externally, yet their representations are inconsistent with the model’s internal embeddings.
- **Dialogue history** accumulates linearly in the context, lacking structure or prioritization.

These memory sources do not share representational formats, structures, or interfaces. As a consequence, **the model cannot integrate them during reasoning.** It may “see” a piece of information but fail to incorporate it into a coherent chain of thought. It may retrieve relevant background but fail to merge it naturally with its internal knowledge.

This structural fragmentation makes reasoning unstable. Sometimes the model appears intelligent. Other times it behaves as if it has completely forgotten essential information. This also explains why reasoning performance often degrades as context length increases or tasks become more complex. The model has no unified memory framework to organize these signals.

From a human perspective, such fragmentation is unnatural. We do not store long-term knowledge, short-term memory, and external information in isolated locations. All of them are organized within a continuous semantic space, and reasoning consists of navigating this space relationally. Yet in current large language models, memory is scattered, and reasoning unfolds only within a narrow, fragmented slice of context.

Therefore, reasoning failures do not occur because the model “does not understand logic”. They occur because the model lacks a memory structure capable of supporting logic consistently.
 To improve reasoning ability, the first problem we must solve is not the “reasoning module” but **the unification of memory.**



## Why Memory and Reasoning Are Deeply Connected

If viewed from a higher perspective, human reasoning is not an independent process. We do not reason without memory, and we cannot make coherent judgments in the absence of relational structure. In fact, nearly all forms of reasoning depend on how memory is organized. Reasoning is not built on logical rules. It is built on memory graphs, semantic structures, and internal representations.

Neuroscience provides clear evidence. When the brain performs reasoning tasks, it does not load a separate “reasoning module”. Instead, it reuses the neural systems already responsible for storing and manipulating memory. The hippocampus uses replay to simulate possible paths. This is a memory mechanism, yet it simultaneously functions as a mechanism for inference. The prefrontal cortex organizes semantic graphs that allow relevant concepts to be activated when needed. This is a memory structure, yet it is also the foundation that enables reasoning to unfold.

Functionally, the key abilities that reasoning depends on are all extensions of memory operations.

- **Activating relevant memories**. The first step of reasoning is not inference but retrieval.
- **Forming connections among memories**. Multi-step reasoning depends on moving through a semantic graph rather than executing formal rules.
- **Maintaining intermediate states**. Working memory allows us to preserve continuity during reasoning.
- **Simulating future scenarios**. Hippocampal replay is both memory extraction and the foundation of reasoning and planning.

Taken together, these processes show that **the structure of memory determines the upper bound of reasoning ability.**
 If memory is sparse and fragmented, reasoning can only be short-range and local. If memory is continuous and operable, reasoning naturally becomes deeper and broader.

From this perspective, reasoning should not be seen as an extra module added on top. It is better understood as a higher-level manifestation of memory mechanisms. When we ask a model to think, explain, or plan, we are not asking it to execute logical rules. We are asking it **to manipulate memory inside its own representational space**.

This leads to a clear insight.
 **To build models with stronger reasoning capabilities, we should not begin with a “reasoning module”. We should begin with memory structure.**
 Only when memory is organized into a unified and coherent space can reasoning become a natural operation on that space rather than a patchwork of techniques.

## The Case for a Unified Latent Memory Space

If memory and reasoning are fundamentally homologous, then the central problem in today’s models is not that their “reasoning ability” is insufficient, but that **memory has been fragmented at its core**. Long-term knowledge is stored in the weights, short-term memory in attention, retrieval-based memory in external databases, and conversational content in the context window. These components do not share a common format, structure, or semantic space. During reasoning, the model must constantly switch among these heterogeneous sources, resulting in a form of “fragmented reasoning” rather than a continuous thought process.

A natural question follows: what happens if these memory sources can be compressed, aligned, and organized into a single latent space? Such a space does not need to be an explicit knowledge base, nor does it require complex symbolic structures. It only needs to satisfy one property: **all forms of memory can be encoded, accessed, and composed in the same way.**

In such a unified space, the model can:

- **Access long-term and short-term memory under the same representation**, eliminating the need to switch between modules.
- **Perform reasoning as direct manipulation of memory**, rather than token-level generation.
- **Integrate new information naturally into existing representations**, rather than attaching it as a temporary patch.
- **Maintain consistency across multi-turn dialogue and cross-task usage**, because the underlying representational structure remains unified.
- **Share memory slots across multiple agents**, enabling genuine latent-level collaboration instead of exchanging text.

From an engineering perspective, the greatest advantage of a unified latent memory space is that it aligns all memory forms behind a single interface. This reduces friction throughout the reasoning process and gives the model, for the first time, an internal memory structure that can be persistently updated and dynamically manipulated.

More importantly, constructing such a unified space does not require any magical architectural breakthrough. Contrastive learning, embedding alignment, latent rewriting, fast weights, and the representational capabilities of modern models already provide realistic and practical mechanisms for building it. In other words, we do not need to wait for a new architectural revolution. Existing techniques are sufficient to support this line of research.

The unified memory space is not an external module bolted onto the model. It is a reorganization of the model’s internal representations. When memory can be stored and accessed in a consistent way, reasoning ceases to be an additional technique and becomes a natural operation on that space.



## The Unified Latent Memory Space

If memory exists within the model in multiple forms, and reasoning depends on how these memories are organized, then it becomes clear that a structure capable of integrating these representations is necessary. We refer to this structure as a **unified latent memory space**: a space where the model’s long-term knowledge, short-term context, newly acquired information, and intermediate states formed during reasoning can all exist in the same format. It is not an external module, but a reorganization of the model’s internal representational structure.

In this space, all memories are represented as operable latent vectors. These vectors can be read by the model, and they can also be modified, rewritten, compressed, or expanded during the reasoning process. Reasoning is no longer a sequential generation of tokens but resembles performing a series of transformations inside this memory space. The final textual output becomes merely a projection of these operations rather than the reasoning process itself.

To enable the model to effectively use this space, we require three essential components:

- **Memory Slots**
   Persistent latent units that store the model’s long-term knowledge, user preferences, compressed contextual representations, and intermediate reasoning results. They form the model’s “internal semantic map” and can expand or update as tasks evolve.

- **Latent Rewriting**
   Every new task, question, or input is not fed into the model directly. Instead, it is first mapped into a memory-friendly latent representation so that it naturally aligns with existing memory. This step allows the model to understand “how the present query relates to what is already stored” before reasoning begins.

- **Memory-Aware Reasoning**
   During reasoning, the model no longer blindly unfolds token-level string generation. Instead, it performs a series of latent-level operations: retrieving relevant memories, creating new connections among them, retaining or discarding intermediate states, and generating replay-like trajectories. The final answer is simply the projection of these latent operations into language.

The most significant shift introduced by the unified latent memory space is that **reasoning is no longer a by-product of language generation; it becomes a native operation within the memory space itself.** When all forms of memory can be accessed and modified in the same underlying format, reasoning naturally gains continuity, depth, and coherence without relying on prompt engineering or external scaffolding.

This space also enables new possibilities for multi-agent collaboration. One agent can write memory into the latent space, and another agent can read it directly without communicating through text. Information transfer becomes more precise, compressed, and controllable, and collaboration can occur through shared memory rather than shared language.

The unified latent memory space is not a distant-future idea. It is highly compatible with existing model architectures, alignment techniques, and latent representation learning methods. More importantly, it provides a path for transforming large models from “language generators” into “memory-driven reasoning systems.”



## **What This Unified Space Enables**

When memory is organized within a single latent space, the model does not gain a “new ability.” Instead, it gains a natural connection between abilities it already possesses. This connection allows reasoning to no longer depend on prompt engineering, long contexts, retrieval tricks, or multi-step guidance. Instead, it relies on the model’s own representational structure to maintain coherence and depth.

First, a unified memory space allows the model to stably access task-relevant long-term knowledge during reasoning. In the past, this knowledge was buried in the weights and difficult to explicitly activate within a specific task. Now, it can appear as memory slots that the model can directly select, align, and compose.

Second, reasoning is no longer constrained by token-level unfolding. The model can execute replay-like trajectories within the latent space, gradually forming intermediate representations and completing deep reasoning without depending on language generation. This makes the reasoning process itself more stable because it is no longer affected by linguistic noise, gradient instability, or prompt variations.

More importantly, the unified space supports a challenge that has long been considered unsolved: cross-turn consistency. When the model stores user preferences, semantic state, and task progress within the same latent space, it can maintain a continuous structure of thought even after dozens of dialogue turns, without relying on manual context concatenation.

This space also enables multiple agents to exchange information without relying on long, ambiguous, or easily misunderstood textual descriptions. Different agents can share memory slots at the latent level, forming a truly implicit collaboration structure. For example, a perception agent can write a compressed observation into the space, and a reasoning agent can directly read it and expand the computation without needing to re-describe the scene or provide additional prompts.

Therefore, a unified memory space introduces several new capability modes:

- **1. Stable multi-step reasoning**
   Reasoning chains are executed inside the latent space without relying on token-level expansion.

- **2. Long-term consistency**
   Memory can persist across turns and tasks rather than being constrained by context length.

- **3. Enhanced knowledge integration**
   Long-term knowledge and short-term information naturally fuse under a shared representation.

- **4. Unified modeling of preferences, identity, and history**
   System-level consistency no longer depends on repetitive system prompts.

- **5. Implicit multi-agent collaboration**
   Agents share latent memory rather than text.

These capabilities are important on their own, but when unified within a single structure, the real change appears in how the model thinks. Reasoning becomes a process of continually rewriting and updating representations inside the unified memory space rather than a sequence of linguistic operations. The model no longer merely generates answers; it forms an internal and stable trajectory of thought.

## A Simple Example: Reasoning Inside a Shared Memory Space

To understand the role of a unified memory space more intuitively, let us consider a simplified example. Suppose there are two models with different abilities: one excels at perception (the Perception Agent), and the other excels at reasoning (the Reasoning Agent). In traditional model setups, collaboration between them typically relies on textual descriptions: the former must “describe” what it observes, and the latter must then “read” this description. This process is lengthy, ambiguous, and prone to losing crucial details.

In a unified latent memory space, this process becomes entirely different.

When the Perception Agent watches a video or reads an image, it does not try to convert everything into language. Instead, it writes the abstracted observation directly into a memory slot. This slot is neither a sentence nor a JSON structure; it is a structured latent representation — compressed, yet highly compatible with the model’s reasoning mechanisms.

Later, when the Reasoning Agent receives a question such as “On which side of the scene is the red object?”, it does not re-watch the video nor rely on verbose linguistic descriptions. Instead, it directly queries the shared latent space for the relevant memory slot. This slot provides not a “description” but an *actionable representation*, enabling the Reasoning Agent to unfold a reasoning trajectory on top of it, generate replay-like internal steps, and ultimately reach the answer.

In this process, several key changes naturally occur:

- **Information is not transmitted through language, eliminating linguistic noise and descriptive loss.**
-  **Reasoning is based on latent structure rather than natural language, making the chain more stable.**
- **Neither agent requires shared prompt engineering or external retrieval.**
- **The memory slot itself contains structure, allowing deep reasoning instead of shallow surface-level description.**

More importantly, this example illustrates a core principle: **collaboration and reasoning do not need to be performed through text; they occur naturally within a unified memory space.**

The same logic applies to a single model. When the model reads a question, it first rewrites the question into a form suitable for the latent space. During reasoning, it repeatedly reads and writes into relevant memory slots, forming an internal pathway of thought. The final latent result is then decoded into a textual output.

What we call “thinking” no longer happens in language. It happens in the memory space. Language is merely the final projection of that process.



## A Research Roadmap for the Next Generation of Memory-Driven Models

If we accept the view that “memory and reasoning are fundamentally the same process,” and if we believe that a unified latent space can serve as their intersection, then a natural question follows: how should this direction be systematically advanced in the future? We can treat this not as a simple technique or module, but as an entirely new modeling paradigm. It touches on how models organize representations, how they execute reasoning, how they accumulate knowledge over time, and how they collaborate with other models.

There are several clear research directions for the coming years.

- First is **how to construct and train the unified memory space itself**. Current model representations already possess rich semantic structure, but they lack an explicit interface that unifies “long-term memory + short-term state + reasoning trajectories.” Methods may include contrastive learning, slot-based representations, latent alignment techniques, fast weight adaptation, or other forms of shared-representation mechanisms.
- Second is **how to perform reasoning inside the latent space**. Traditional reasoning relies on linguistic unfolding, and language introduces noise, ambiguity, and unnecessary constraints. We need to explore how models can generate replay-like trajectories in latent space, perform structured operations, build intermediate states, and extract final outputs. This is key to making reasoning stable, deep, and sustainable.
- Third is **integrating long-term and short-term memory**. A genuinely intelligent system must not only access long-term knowledge but also update, compress, and rewrite memory during tasks. A unified latent space makes this possible, but ensuring stability, preventing catastrophic forgetting, and maintaining consistency remain open challenges.
- Fourth is **modeling preferences, identity, and user state**. Today’s models lack consistency across conversations and tasks. A unified space enables continuous, structured storage for user-level memory, opening a realistic path toward long-term personalization and making model behavior more reliable and controllable.
- Finally, **shared memory spaces for multiple agents**. If multiple models can share memory slots at the latent level, collaboration no longer relies on textual communication but occurs directly at the representational layer. This enables new forms of agent architectures, planning mechanisms, and collaboration strategies, allowing multi-model systems to exhibit early forms of “team intelligence.”

These directions do not require reinventing deep learning. They are highly compatible with ongoing progress in representation learning, fine-tuning methods, alignment techniques, latent reasoning, fast weights, and even tool-usage systems. More importantly, they all point toward the same overarching goal: transforming models from “language generators” into “reasoning systems built on continuous memory structures.”

A new paradigm is emerging, and the unified latent memory space may become its foundational layer.



## Toward a Memory-Centric Paradigm

When we look back at the development of large-scale models, we can observe a clear trend: whether it is expanding parameter size, adding chain-of-thought reasoning, incorporating retrieval, building tool ecosystems, or enforcing consistency across multiple tasks, all these efforts ultimately point to the same bottleneck — the model’s memory structure. We have tried to improve performance with more knowledge, better reasoning techniques, and longer contexts, but these methods cannot escape a fundamental fact: **the model does not have a unified memory framework capable of supporting reasoning, learning, and collaboration.**

Once we shift our perspective from “improving reasoning ability” to “reshaping the memory structure,” many issues that were previously difficult to explain become much clearer: Why does a model fail to reason even when it knows the correct facts? Why do contradictions increase as conversations grow longer? Why is multi-agent collaboration unstable? Why is preference injection inconsistent across tasks? These failures are not due to flaws in logic; they arise because the organization of memory limits the depth and scope of the model’s thinking.

A memory-centric paradigm means that reasoning is no longer viewed as an ability parallel to memory, but as an ability built on top of memory. The more unified, more operable, and more expressive the memory structure becomes, the more natural, stable, and powerful reasoning becomes. The model’s behavior is no longer a reaction to prompts but an adjustment and rewriting of its own internal representational state.

Under such a paradigm, the core questions of the future will no longer be:

- **“How do we make the model write longer chains of thought?”**
-  **“How do we make the model remember more facts?”**
-  **“How do we make the model understand more task formats?”**

Instead, the questions become more fundamental:

- **How should the model’s memory be organized?**
-  **How can reasoning occur in latent space in the most natural way?**
-  **Can different forms of knowledge, preferences, and experiences share the same structure?**
-  **Can multiple agents collaborate within a shared memory space?**

Once these questions begin to be discussed systematically, a new research paradigm has already emerged:
 **the goal is not to build models that are bigger, faster, or better at generating text, but to build models with better memory structures.**

The growth of model capability has never been linear; it has always depended on paradigm shifts. Language modeling solved the problem of representation, pretraining solved the problem of knowledge, and instruction tuning solved the problem of alignment. The next shift will likely come from rethinking the relationship between memory and reasoning — and from the practical construction of a unified memory space.



## Closing: Unifying Memory and Reasoning

From a surface-level perspective, many deficiencies of large language models — unstable reasoning, shifting preferences, difficulty maintaining coherence across multiple turns, and information loss in multi-agent collaboration — appear unrelated. Yet when we reexamine these issues from a mechanistic perspective, it becomes evident that they all point to the same fundamental cause: the model’s memory structure has never been unified. Reasoning unfolds on top of fragmented memory, and thus inevitably exhibits fragmented behavior.

In natural intelligence, the situation is the opposite. Memory is organized, accessed, and updated within a unified structure, and reasoning is an operation performed on this structure. When we bring this perspective into model design, many problems that previously seemed complex become much clearer: reasoning is not a linguistic trick but a form of memory manipulation; consistency does not come from prompts but from unified representations; collaboration is not the exchange of text but the sharing of representations.

A unified latent memory space is therefore not merely an improvement. It represents a shift in foundational assumptions. It enables models to process knowledge, context, preferences, and experience in the same representational form, allowing reasoning to emerge naturally as structured interactions among these memories, and enabling multi-model systems to possess, for the first time, a truly shared semantic substrate.

When we move beyond the framing of “language generators” and begin to interpret models as “memory-driven reasoning systems,” the next generation of intelligent capabilities begins to take shape.

**In the end, reasoning is simply what happens when memory is organized well enough.**





# Citation

Please cite this work as:

```
Feng, He. "Why should memory and reasoning be unified?". Feng'Log (Dec 2025). https://hefengcs.github.io/posts/2025-12-10-memory/
```

Or use the BibTex citation:

```
@article{feng2025memory,
  title = {Why should memory and reasoning be unified?},
  author = {Feng, He},
  journal = {hefengcs.github.io},
  year = {2025},
  month = {December},
  url = "https://hefengcs.github.io/posts/2025-12-10-memory/"
}
```



# References

[1] Tolman, E. C. (1948). Cognitive maps in rats and men. *Psychological Review*, *55*(4), 189.

[2] Eichenbaum, H. (2015). The hippocampus as a cognitive map… of social space. *Neuron*, *87*(1), 9–11.

[3] Pfeiffer, B. E. (2020). The content of hippocampal “replay”. *Hippocampus*, *30*(1), 6–18.

[4] Whittington, J. C., Muller, T. H., Mark, S., Chen, G., Barry, C., Burgess, N., & Behrens, T. E. (2020). The Tolman-Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. *Cell*, *183*(5), 1249–1263.

[5] Egner, T. (2009). Prefrontal cortex and cognitive control: motivating functional hierarchies. *Nature Neuroscience*, *12*(7), 821–822.

[6] LeCun, Y. (2022). A path towards autonomous machine intelligence version 0.9.2, 2022-06-27. *Open Review*, *62*(1), 1–62.

[7] Setlur, A., Rajaraman, N., Levine, S., & Kumar, A. Scaling Test-Time Compute Without Verification or RL is Suboptimal. In *ICLR 2025 Workshop: VerifAI: AI Verification in the Wild*.

[8] Chhikara, P., Khant, D., Aryan, S., Singh, T., & Yadav, D. (2025). Mem0: Building production-ready AI agents with scalable long-term memory. *arXiv preprint arXiv:2504.19413*.

[9] Packer, C., Fang, V., Patil, S., Lin, K., Wooders, S., & Gonzalez, J. (2023). MemGPT: Towards LLMs as Operating Systems.

[10] Du, Z., Wang, R., Bai, H., Cao, Z., Zhu, X., Zheng, B., ... & Ying, H. (2025). Enabling Agents to Communicate Entirely in Latent Space. *arXiv preprint arXiv:2511.09149*.

[11] Zhang, G., Fu, M., & Yan, S. (2025). MemGen: Weaving generative latent memory for self-evolving agents. *arXiv preprint arXiv:2509.24704*.

[12] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023, December). Direct preference optimization: Your language model is secretly a reward model. In *Proceedings of the 37th International Conference on Neural Information Processing Systems* (pp. 53728–53741).

[13] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems*, *30*.

[14] Kori, A., Locatello, F., Santhirasekaram, A., Toni, F., Glocker, B., & De Sousa Ribeiro, F. (2024). Identifiable object-centric representation learning via probabilistic slot attention. *Advances in Neural Information Processing Systems*, *37*, 93300–93335.

[15] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning* (pp. 8748–8763). PMLR.

[16] Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, November). A simple framework for contrastive learning of visual representations. In *International Conference on Machine Learning* (pp. 1597–1607). PMLR.

[17] Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., ... & Krishnan, D. (2020). Supervised contrastive learning. *Advances in Neural Information Processing Systems*, *33*, 18661–18673.

[18] Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. Compressive Transformers for Long-Range Sequence Modelling. In *International Conference on Learning Representations*.

[19] Liu, J., & Mozafari, B. (2024). Query rewriting via large language models. *arXiv preprint arXiv:2403.09060*.

[20] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. *Advances in neural information processing systems*, *35*, 24824-24837.

[21] Garg, S., Tsipras, D., Liang, P. S., & Valiant, G. (2022). What can transformers learn in-context? a case study of simple function classes. *Advances in neural information processing systems*, *35*, 30583-30598.

[22] Kandpal, N., Deng, H., Roberts, A., Wallace, E., & Raffel, C. (2023, July). Large language models struggle to learn long-tail knowledge. In *International conference on machine learning* (pp. 15696-15707). PMLR.
